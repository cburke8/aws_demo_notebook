{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24626377c1e14b388cf4661d5d1da8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1605539343869_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-150.us-east-2.compute.internal:20888/proxy/application_1605539343869_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-152.us-east-2.compute.internal:8042/node/containerlogs/container_1605539343869_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as psf\n",
    "\n",
    "import os\n",
    "#test change again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b40fb9b0f764673aa60f5110fdac96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"DataOps\").config(\"hive.metastore.client.factory.class\",\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\").enableHiveSupport().getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e53afa6bc72430aa52375bf3ab80153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bronze_path = 's3://citi-bike-system-data-new/bronze/'\n",
    "silver_path = 's3://citi-bike-system-data-new/silver/'\n",
    "gold_path   = 's3://citi-bike-system-data-new/gold/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae02fdd34a7542a9ad7c7827849d06c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o114.csv.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 5E87688D0BF834C2; S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=; Proxy: null), S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.listStatus(S3NativeFileSystem.java:606)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.listStatus(S3NativeFileSystem.java:577)\n",
      "\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)\n",
      "\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)\n",
      "\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n",
      "\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2031)\n",
      "\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2010)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:499)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:245)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.globPathIfNecessary(SparkHadoopUtil.scala:255)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:549)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.flatMap(List.scala:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 5E87688D0BF834C2; S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=; Proxy: null), S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1811)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1395)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1371)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:22)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:8)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)\n",
      "\t... 33 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 476, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o114.csv.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 5E87688D0BF834C2; S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=; Proxy: null), S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.listStatus(S3NativeFileSystem.java:606)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.listStatus(S3NativeFileSystem.java:577)\n",
      "\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)\n",
      "\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)\n",
      "\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n",
      "\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2031)\n",
      "\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2010)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:499)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:245)\n",
      "\tat org.apache.spark.deploy.SparkHadoopUtil.globPathIfNecessary(SparkHadoopUtil.scala:255)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:549)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.immutable.List.flatMap(List.scala:355)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 5E87688D0BF834C2; S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=; Proxy: null), S3 Extended Request ID: JxAT5cKMi4qwSnlx/cHQp1JWg9XTM3NNc4LM80UVfJR9kVPCerUdY9se9E8/yDCAqfEOnVRSdXQ=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1811)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1395)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1371)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:22)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:8)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)\n",
      "\t... 33 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_schema = StructType([\n",
    "  StructField('tripduration', IntegerType()),\n",
    "  StructField('start_time', StringType()),\n",
    "  StructField('stop_time',  StringType()),\n",
    "  StructField('start_station_id', IntegerType()),\n",
    "  StructField('start_station_name', StringType()),\n",
    "  StructField('start_station_latitude', StringType()),\n",
    "  StructField('start_station_longitude', StringType()),\n",
    "  StructField('end_station_id', IntegerType()),\n",
    "  StructField('end_station_name', StringType()),\n",
    "  StructField('end_station_latitude', StringType()),\n",
    "  StructField('end_station_longitude', StringType()),\n",
    "  StructField('bike_id', IntegerType()),\n",
    "  StructField('user_type', StringType()),\n",
    "  StructField('birth_year', StringType()),\n",
    "  StructField('user_gender', StringType()),\n",
    "  ])\n",
    "  \n",
    "# read the raw trip history data to dataframe, without triggering job, by passing csv schema\n",
    "bronze_all_csv = bronze_path + '*'\n",
    "\n",
    "bronzeDF = spark.read.csv(\n",
    "  bronze_all_csv, \n",
    "  header=True,\n",
    "  schema=trip_schema\n",
    "  )\n",
    "  \n",
    "#bronze.rdd.getNumPartitions()\n",
    "#bronze = bronze.coalesce(200) # If we use Coalesce to reduce NumPartitions, then we can use Coalesce to INCREASE numPart till that old num !!\n",
    "#bronze = bronze.repartition(201, [\"start_station_id\", \"end_station_id\"])\n",
    "\n",
    "bronzeDF = bronzeDF.distinct()\n",
    "print(bronzeDF.orderBy(bronzeDF.start_time.desc()))\n",
    "\n",
    "bronzeDF.write.format('parquet').mode('overwrite').save(silver_path)\n",
    "\n",
    "slvrDF = spark.read.format('parquet').load(silver_path)\n",
    "\n",
    "# Validation step\n",
    "# Verify Counts/Schema/Top few records etc.\n",
    "print(\"slvrDF.count()  :   \", slvrDF.count())\n",
    "print(\"================================================\")\n",
    "print(\"slvrDF.dtypes  :   \", slvrDF.dtypes)\n",
    "print(\"================================================\")\n",
    "print(\"slvrDF.head()  :   \", slvrDF.head())\n",
    "print(\"================================================\")\n",
    "print(\"slvrDF.schema  :   \", slvrDF.schema)\n",
    "\n",
    "slvrDF.createOrReplaceTempView(\"silver\")\n",
    "\n",
    "# Making DIM_STATION starts:\n",
    "# 2 start cols:\n",
    "startDF = slvrDF.select([\"start_station_id\",\"start_station_name\"]).distinct().withColumnRenamed(\"start_station_id\",\"station_id\").withColumnRenamed(\"start_station_name\",\"station_name\")\n",
    "# 2 end cols:\n",
    "endDF = slvrDF.select([\"end_station_id\",\"end_station_name\"]).distinct().withColumnRenamed(\"end_station_id\",\"station_id\").withColumnRenamed(\"end_station_name\",\"station_name\")\n",
    "# Merge them into dim station DF:\n",
    "unionDF = startDF.unionAll(endDF).distinct()\n",
    "\n",
    "#display(unionDF)\n",
    "unionDF.write.format('parquet').mode('overwrite').save(f\"{gold_path}dim_station\")\n",
    "\n",
    "# Create Managed Table using spark SQL (alternative to Hive SQL)\n",
    "tableName = \"dim_station\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

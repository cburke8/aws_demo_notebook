{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as psf\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test commit\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"DataOps\").config(\"hive.metastore.client.factory.class\",\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "print(\"Session Started as Expected\")\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "print(\"sqlContext initialized\")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = 's3://citi-bike-system-data-new/bronze/'\n",
    "silver_path = 's3://citi-bike-system-data-new/silver/'\n",
    "gold_path   = 's3://citi-bike-system-data-new/gold/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_schema = StructType([\n",
    "  StructField('tripduration', IntegerType()),\n",
    "  StructField('start_time', StringType()),\n",
    "  StructField('stop_time',  StringType()),\n",
    "  StructField('start_station_id', IntegerType()),\n",
    "  StructField('start_station_name', StringType()),\n",
    "  StructField('start_station_latitude', StringType()),\n",
    "  StructField('start_station_longitude', StringType()),\n",
    "  StructField('end_station_id', IntegerType()),\n",
    "  StructField('end_station_name', StringType()),\n",
    "  StructField('end_station_latitude', StringType()),\n",
    "  StructField('end_station_longitude', StringType()),\n",
    "  StructField('bike_id', IntegerType()),\n",
    "  StructField('user_type', StringType()),\n",
    "  StructField('birth_year', StringType()),\n",
    "  StructField('user_gender', StringType()),\n",
    "  ])\n",
    "  \n",
    "# read the raw trip history data to dataframe, without triggering job, by passing csv schema\n",
    "bronze_all_csv = bronze_path + '*'\n",
    "\n",
    "bronzeDF = spark.read.csv(\n",
    "  bronze_all_csv, \n",
    "  header=True,\n",
    "  schema=trip_schema\n",
    "  )\n",
    "  \n",
    "#bronze.rdd.getNumPartitions()\n",
    "#bronze = bronze.coalesce(200) # If we use Coalesce to reduce NumPartitions, then we can use Coalesce to INCREASE numPart till that old num !!\n",
    "#bronze = bronze.repartition(201, [\"start_station_id\", \"end_station_id\"])\n",
    "\n",
    "bronzeDF = bronzeDF.distinct()\n",
    "print(bronzeDF.orderBy(bronzeDF.start_time.desc()))\n",
    "\n",
    "bronzeDF.write.format('parquet').mode('overwrite').save(silver_path)\n",
    "\n",
    "slvrDF = spark.read.format('parquet').load(silver_path)\n",
    "\n",
    "# Validation step\n",
    "# Verify Counts/Schema/Top few records etc.\n",
    "print(\"slvrDF.count()  :   \", slvrDF.count())\n",
    "print(\"================================================\")\n",
    "print(\"slvrDF.dtypes  :   \", slvrDF.dtypes)\n",
    "print(\"================================================\")\n",
    "print(\"slvrDF.head()  :   \", slvrDF.head())\n",
    "print(\"================================================\")\n",
    "print(\"slvrDF.schema  :   \", slvrDF.schema)\n",
    "\n",
    "slvrDF.createOrReplaceTempView(\"silver\")\n",
    "\n",
    "# Making DIM_STATION starts:\n",
    "# 2 start cols:\n",
    "startDF = slvrDF.select([\"start_station_id\",\"start_station_name\"]).distinct().withColumnRenamed(\"start_station_id\",\"station_id\").withColumnRenamed(\"start_station_name\",\"station_name\")\n",
    "# 2 end cols:\n",
    "endDF = slvrDF.select([\"end_station_id\",\"end_station_name\"]).distinct().withColumnRenamed(\"end_station_id\",\"station_id\").withColumnRenamed(\"end_station_name\",\"station_name\")\n",
    "# Merge them into dim station DF:\n",
    "unionDF = startDF.unionAll(endDF).distinct()\n",
    "\n",
    "#display(unionDF)\n",
    "unionDF.write.format('parquet').mode('overwrite').save(f\"{gold_path}dim_station\")\n",
    "\n",
    "# Create Managed Table using spark SQL (alternative to Hive SQL)\n",
    "tableName = \"dim_station\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
